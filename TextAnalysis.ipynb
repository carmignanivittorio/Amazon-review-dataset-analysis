{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "import pickle\n",
    "import pyprind\n",
    "import re\n",
    "from PIL import Image\n",
    "from IPython.core.display import Image as img\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import math\n",
    "from prettytable import PrettyTable as PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_DATA = \"V:/Programmazione/Amazon/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"%sAmazonDataProject.pkl\" % PATH_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproccessing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def percent(value,tot):\n",
    "    return value/float(tot)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering of categories with less then 1000 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATEGORIES KEEPED (20): Baby, Books, Camera, Digital_Ebook_Purchase, Digital_Music_Purchase, Digital_Video_Download, Electronics, Home, Mobile_Apps, Music, Musical Instruments, PC, Shoes, Sports, Toys, Video, Video DVD, Video Games, Watches, Wireless\n",
      "WE ARE KEEPING 1702443/1705765 (99.81%) REVIEWS\n",
      "\n",
      "CATEGORIES REMOVED (13): Apparel, Automotive, Beauty, Health & Personal Care, Home Entertainment, Home Improvement, Kitchen, Lawn and Garden, Luggage, Office Products, Personal_Care_Appliances, Pet Products, Software\n",
      "WE ARE DISCARDING 3322/1705765 (0.19%) REVIEWS\n"
     ]
    }
   ],
   "source": [
    "categories = sorted(df.product_category.unique().tolist())\n",
    "tempCategories = []\n",
    "delCategories = []\n",
    "N = 1000 #inferior limit\n",
    "for c in categories:\n",
    "    if(df[df.product_category==c].count()[0])>=N:\n",
    "        tempCategories.append(c)\n",
    "    else:\n",
    "        delCategories.append(c)\n",
    "tot = df.count()[0]\n",
    "NKeep = df[df['product_category'].isin(tempCategories)].count()[0]\n",
    "NDisc = tot-NKeep\n",
    "print(\"CATEGORIES KEEPED (%d): %s\" % (len(tempCategories),\", \".join(tempCategories)))\n",
    "print(\"WE ARE KEEPING %d/%d (%.2f%%) REVIEWS\" % (NKeep,tot,percent(NKeep,tot)))\n",
    "print(\"\\nCATEGORIES REMOVED (%d): %s\" % (len(delCategories),\", \".join(delCategories)))\n",
    "print(\"WE ARE DISCARDING %d/%d (%.2f%%) REVIEWS\" % (NDisc,tot,percent(NDisc,tot)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = tempCategories\n",
    "df = df[df['product_category'].isin(categories)] #keep only the reviews of products contained into \"survival\" categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deletion of stopwords from reviews' bodies and headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english')).union([u'br'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given a the a string the function return the tokenization of the string, a list which contain all the words transformed in lowercase\n",
    "\"\"\"\n",
    "def tokenize(text):\n",
    "    return [t.lower() for t in nltk.word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given a the list of strings the function return a list which contain all the words of the reviews which:\n",
    "1. are not stopwords\n",
    "2. contain only letters\n",
    "\"\"\"\n",
    "def deleteStopwords(text):\n",
    "    return [t for t in text if(not bool(re.search(r\"[^A-Za-z]\", t)) and t not in STOPWORDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_rev = pd.read_pickle(\"%sReviews.pkl\" % PATH_DATA)\n",
    "    df_rev_stop = pd.read_pickle(\"%sReviews_without_stopwords.pkl\" % PATH_DATA)\n",
    "except IOError as I:\n",
    "    df_rev = df.filter([\"review_id\",\"review_headline\",\"review_body\"], axis=1) #keep only the \"interesting\" columns\n",
    "    df_rev[\"review_headline\"] = df_rev[\"review_headline\"].apply(func = tokenize) #apply the function deleteStopwords on headlines\n",
    "    df_rev[\"review_body\"] = df_rev[\"review_body\"].apply(func = tokenize) #apply the function deleteStopwords on bodies\n",
    "    df_rev.to_pickle(\"%sReviews.pkl\" % PATH_DATA) #save the file\n",
    "    print(df_rev.head())\n",
    "    df_rev_stop = df_rev.filter([\"review_id\",\"review_headline\",\"review_body\"], axis=1)\n",
    "    df_rev_stop[\"review_headline\"] = df_rev_stop[\"review_headline\"].apply(func = deleteStopwords) #apply the function deleteStopwords on headlines\n",
    "    df_rev_stop[\"review_body\"] = df_rev_stop[\"review_body\"].apply(func = deleteStopwords) #apply the function deleteStopwords on bodies\n",
    "    df_rev_stop.to_pickle(\"%sReviews_without_stopwords.pkl\" % PATH_DATA) #save the file\n",
    "    print(df_rev_stop.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_rev_stop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction of \"large\" documents divided by categories/star ratings, body/headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large document of categories' bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_reviews = dict()\n",
    "all_reviews_without_stopwords = []\n",
    "for c in categories:\n",
    "    categories_reviews[c] = []\n",
    "    for index,row in df[df.product_category==c].iterrows():\n",
    "        categories_reviews[c]+= df_rev_stop.loc[index].review_body\n",
    "    all_reviews_without_stopwords+=categories_reviews[c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large document of categories' headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_reviews_headlines = dict()\n",
    "for c in categories:\n",
    "    categories_reviews_headlines[c] = []\n",
    "    for index,row in df[df.product_category==c].iterrows():\n",
    "        categories_reviews_headlines[c]+=df_rev_stop.loc[index].review_headline\n",
    "    all_reviews_without_stopwords+=categories_reviews_headlines[c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large document of star ratings' bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stars_body = dict()\n",
    "for i in range(1,6):\n",
    "    stars_body[i] = []\n",
    "    for index,row in df[df.star_rating==i].iterrows():\n",
    "        stars_body[i]+=df_rev_stop.loc[index].review_body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large document of star ratings' headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stars_head = dict()\n",
    "for i in range(1,6):\n",
    "    stars_head[i] = []\n",
    "    for index,row in df[df.star_rating==i].iterrows():\n",
    "        stars_head[i]+=df_rev_stop.loc[index].review_headline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats on the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = PT(['Category', 'Number of reviews','% respect to the total'])\n",
    "tot = float(df.count()[0])\n",
    "for c in categories:\n",
    "    count = df[df.product_category==c].count()[0]\n",
    "    t.add_row([c,count,\"%.2f%%\" % percent(count,tot)])\n",
    "t.align[\"Category\"] = 'l'\n",
    "print(\"Number of total reviews = %.0f\" % tot)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = PT(['Stars', 'Number of reviews','% respect to the total'])\n",
    "for i in range(1,6):\n",
    "    count = df[df.star_rating==i].count()[0]\n",
    "    t.add_row([i,count,\"%.2f%%\" % percent(count,tot)])\n",
    "print(\"Number of total reviews = %.0f\" % tot)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = PT(['Category', 'Body','Body-stopwords','% del Body', 'Headline', 'Headline-stopwords','% del Headline'])\n",
    "tot = 0\n",
    "for c in categories:\n",
    "    body_tot = len(np.concatenate(df[df.product_category==c].review_body.tolist())))\n",
    "    body_stop = len(categories_reviews[c])\n",
    "    head_tot = len(nltk.word_tokenize(\" \".join(df[df.product_category==c].review_headline.tolist())))\n",
    "    head_stop = len(categories_reviews_headlines[c])\n",
    "    t.add_row([c,\n",
    "               body_tot,\n",
    "               body_stop,\n",
    "               \"%.2f%%\" % percent(body_tot-body_stop,body_tot),\n",
    "               head_tot,\n",
    "               head_stop,\n",
    "               \"%.2f%%\" % percent(head_tot-head_stop,head_tot),\n",
    "              ])\n",
    "    tot+=body_tot+head_tot\n",
    "print(\"Number of words (headline+body) of all reviews = %d, without stopwrods =%d, %% deleted = %.2f%%\" % (tot,len(all_reviews_without_stopwords),percent(tot-len(all_reviews_without_stopwords),tot)))\n",
    "t.align[\"Category\"] = 'l'\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating stars' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = PT(['Stars', 'Body','Body-stopwords','% del Body', 'Headline', 'Headline-stopwords','% del Headline'])\n",
    "for i in range(1,6):\n",
    "    body_tot = len(nltk.word_tokenize(\" \".join(df_rev[df.star_rating==i].review_body.tolist())))\n",
    "    body_stop = len(stars_body[i])\n",
    "    head_tot = len(nltk.word_tokenize(\" \".join(df[df.star_rating==i].review_headline.tolist())))\n",
    "    head_stop = len(stars_head[i])\n",
    "    t.add_row(['*'*i,\n",
    "               body_tot,\n",
    "               body_stop,\n",
    "               \"%.2f%%\" % percent(body_tot-body_stop,body_tot),\n",
    "               head_tot,\n",
    "               head_stop,\n",
    "               \"%.2f%%\" % percent(head_tot-head_stop,head_tot),\n",
    "              ])\n",
    "print(\"Number of words (headline+body) of all reviews = %d, without stopwrods =%d, %% deleted = %.2f%%\" % (tot,len(all_reviews_without_stopwords),percent(tot-len(all_reviews_without_stopwords),tot)))\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories' graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotFreq(list_words,subplot_column,title):\n",
    "    fr = nltk.FreqDist(list_words)\n",
    "    plt.subplot(1, 2, subplot_column)\n",
    "    plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "    plt.title(title, fontsize = 15)\n",
    "    fr.plot(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in categories:\n",
    "    plotFreq(categories_reviews[c],1,\"%s body\" % c)\n",
    "    plotFreq(categories_reviews_headlines[c],2,\"%s headline\" % c)\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,6):\n",
    "    plotFreq(stars_body[i],1,\"%s/5 stars body\" % c)\n",
    "    plotFreq(stars_head[i],2,\"%s/5 stars headline\" % c)\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(text)/len(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = PT(['Category', 'Body', 'Headline'])\n",
    "for c in categories:\n",
    "    t.add_row([c,lexical_diversity(categories_reviews[c]),lexical_diversity(categories_reviews_headlines[c])])\n",
    "t.align[\"Category\"] = 'l'\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder2 = BigramCollocationFinder.from_words(all_reviews_without_stopwords)\n",
    "finder2.apply_freq_filter(3) ##ignoring all bigrams which occur less than three times in the corpus\n",
    "res2 = finder2.nbest(bigram_measures.pmi, 10)\n",
    "print(\"TOP BIGRAMS: %s\" % \", \".join([\" \".join(r) for r in res2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder3 = TrigramCollocationFinder.from_words(all_reviews_without_stopwords)\n",
    "finder3.apply_freq_filter(3) ##ignoring all trigrams which occur less than three times in the corpus\n",
    "res3 = finder3.nbest(trigram_measures.pmi, 10)\n",
    "print(\"TOP TRIGRAMS: %s\" % \", \".join([\" \".join(r) for r in res3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams (body+headline) with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = PT(['Category (body+headline)', '1st', '2nd', '3rd','4th'])\n",
    "for c in categories:\n",
    "    finder = BigramCollocationFinder.from_words(nltk.word_tokenize(\" \".join(df[df.product_category==c].review_body.tolist()+df[df.product_category==c].review_headline.tolist())))\n",
    "    finder.apply_freq_filter(3) ##ignoring all bigrams which occur less than three times in the corpus\n",
    "    res = list(finder.nbest(bigram_measures.pmi, 4))\n",
    "    res = [\" \".join(r) for r in res]\n",
    "    t.add_row([c,res[0],res[1],res[2],res[3]])\n",
    "t.align[\"Category\"] = 'l'\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams (body+headline) without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = PT(['Category (body+headline)', '1st', '2nd', '3rd','4th'])\n",
    "for c in categories:\n",
    "    finder = BigramCollocationFinder.from_words(categories_reviews[c]+categories_reviews_headlines[c])\n",
    "    finder.apply_freq_filter(3) ##ignoring all bigrams which occur less than three times in the corpus\n",
    "    res = list(finder.nbest(bigram_measures.pmi, 4))\n",
    "    res = [\" \".join(r) for r in res]\n",
    "    t.add_row([c,res[0],res[1],res[2],res[3]])\n",
    "t.align[\"Category\"] = 'l'\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams (body+headline) with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = PT(['Category (body+headline)', '1st', '2nd', '3rd'])\n",
    "for c in categories:\n",
    "    finder3 = TrigramCollocationFinder.from_words(nltk.word_tokenize(\" \".join(df[df.product_category==c].review_body.tolist()+df[df.product_category==c].review_headline.tolist())))\n",
    "    finder3.apply_freq_filter(3) ##ignoring all trigrams which occur less than three times in the corpus\n",
    "    res = list(finder3.nbest(trigram_measures.pmi, 3))\n",
    "    res = [\" \".join(r) for r in res]\n",
    "    t.add_row([c,res[0],res[1],res[2]])\n",
    "t.align[\"Category\"] = 'l'\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams (body+headline) without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = PT(['Category (body+headline)', '1st', '2nd', '3rd'])\n",
    "for c in categories:\n",
    "    finder3 = TrigramCollocationFinder.from_words(categories_reviews[c]+categories_reviews_headlines[c])\n",
    "    finder3.apply_freq_filter(3) ##ignoring all trigrams which occur less than three times in the corpus\n",
    "    res = list(finder3.nbest(trigram_measures.pmi, 3))\n",
    "    res = [\" \".join(r) for r in res]\n",
    "    t.add_row([c,res[0],res[1],res[2]])\n",
    "t.align[\"Category\"] = 'l'\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common words vs TF_IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfIdf(fdistList):\n",
    "    N_DOCUMENTS = len(fdistList)\n",
    "    print(\"Total number of documents = %d\" % N_DOCUMENTS)\n",
    "    vocab = set()\n",
    "    print(\"I'm creating the dictionary\")\n",
    "    for fdist in fdistList.values():\n",
    "        vocab = vocab.union(set(fdist.keys()))#all words presents in all the documents\n",
    "    idf = dict()\n",
    "    print(\"I'm calculating the IDF\")\n",
    "    for word in vocab:\n",
    "        count = 0.0\n",
    "        for fdist in fdistList.values():\n",
    "            if(word in fdist):\n",
    "                count+=1\n",
    "        idf[word] = math.log(N_DOCUMENTS/count)\n",
    "    listTfIdf = dict()\n",
    "    print(\"I'm calculating the tf-idf\")\n",
    "    i = 0\n",
    "    for title, fdist in fdistList.items():\n",
    "        listTfIdf[title] = dict()\n",
    "        n_token_document = float(sum(fdist.values()))\n",
    "        for word in fdist:\n",
    "            listTfIdf[title][word] = fdist[word] * idf[word] /  n_token_document\n",
    "        i+=1\n",
    "    print(\"I have finished\")\n",
    "    return listTfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to draw the wordcloud\n",
    "def make_word_cloud(dictWords, mask_path=None, background_color = \"white\", max_words = 2000, title = \"\", mux = 1, subplots_params = (1,1,1)):\n",
    "    if(max_words>len(dictWords)):\n",
    "        max_words = len(dictWords)\n",
    "    topK = sorted(dictWords.iteritems(), key = lambda (k, v): (v, k), reverse = True)[:max_words]\n",
    "    text =[] \n",
    "    for word, value in topK:\n",
    "        text+=[word]*int(round(value*mux))\n",
    "    text = \" \".join(text)\n",
    "    if(mask_path!=None):\n",
    "        # read the mask image\n",
    "        mask = np.array(Image.open(mask_path))\n",
    "        wc = WordCloud(background_color = background_color, max_words = max_words,  mask = mask, stopwords = None, collocations = False, max_font_size = 120)\n",
    "        \n",
    "    else:\n",
    "        wc = WordCloud(background_color = background_color, max_words = max_words, stopwords = None, collocations = False, max_font_size = 120)\n",
    "    # generate word cloud\n",
    "    wc.generate(text)\n",
    "    if(mask_path!=None):\n",
    "        wc.recolor(color_func=ImageColorGenerator(mask))\n",
    "    # store to file\n",
    "    plt.subplot(subplots_params[0], subplots_params[1], subplots_params[2])\n",
    "    plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "    plt.title(title, fontsize = 15)\n",
    "    plt.imshow(wc, interpolation = 'bilinear')\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculations of most common words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw wordlcloud taking into account all the categories (body+headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = nltk.FreqDist(all_reviews_without_stopwords)\n",
    "make_word_cloud({v:k for v,k in freq.most_common(300)}, max_words = 300, title = \"Most common word taking into account all categories together\", mask_path = \"ImagesWordClouds/amazon.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of most important words based on the TF-IDF for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist_categories = dict()\n",
    "for c in categories:\n",
    "    freq_dist_categories[c] = nltk.FreqDist(categories_reviews[c]+categories_reviews_headlines[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listTfIdf_categories = tfIdf(freq_dist_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences between Most common and TF-IDF words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the differences between them (body+headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mux = len(all_reviews_without_stopwords) #This will multiply the tf-idf to make them bigger than 0., otherwise is not possible to create the text for the wordcloud.\n",
    "n = 300\n",
    "for c in categories:\n",
    "    make_word_cloud({v:k for v,k in freq_dist_categories[c].most_common(n)}, max_words = n, title = \"Most common words in %s\" % c, mask_path = \"ImagesWordClouds/%s.jpg\" % c,subplots_params=(1,2,1))\n",
    "    make_word_cloud(listTfIdf_categories[c], max_words = n, title = \"TF-IDF words in %s\" % c, mux = mux, mask_path = \"ImagesWordClouds/%s.jpg\" % c, subplots_params=(1,2,2))\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF body vs TF-IDF headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist_categories_bodies = dict()\n",
    "for c in categories:\n",
    "    freq_dist_categories_bodies[c] = nltk.FreqDist(categories_reviews[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listTfIdf_bodies = tfIdf(freq_dist_categories_bodies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist_categories_headlines = dict()\n",
    "for c in categories:\n",
    "    freq_dist_categories_headlines[c] = nltk.FreqDist(categories_reviews_headlines[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listTfIdf_headlines = tfIdf(freq_dist_categories_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mux = len(all_reviews_without_stopwords) #This will multiply the tf-idf to make them bigger than 0., otherwise is not possible to create the text for the wordcloud.\n",
    "n = 300\n",
    "for c in categories:\n",
    "    make_word_cloud(listTfIdf_bodies[c], max_words = n, title = \"TF-IDF body in %s\" % c, mux = mux, mask_path = \"ImagesWordClouds/%s.jpg\" % c, subplots_params=(1,2,1))\n",
    "    make_word_cloud(listTfIdf_headlines[c], max_words = n, title = \"TF-IDF headlline in %s\" % c, mux = mux, mask_path = \"ImagesWordClouds/%s.jpg\" % c, subplots_params=(1,2,2))\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words based on the star_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect to find words which has a sentiment value based on the star rating. For example, for the reviews of 1 start, we expect to find: \"bad, worst..\". Moreover, we forcast to have better more interesting results using the most common words approach compare to the tf-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the reviews' body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_dist_stars_body = dict()\n",
    "for i in range(1,6):\n",
    "    freq_dist_stars_body[i] = nltk.FreqDist(stars_body[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listTfIdf_stars_body = tfIdf(freq_dist_stars_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mux = len(all_reviews_without_stopwords) \n",
    "n = 200\n",
    "for i in range(1,6):\n",
    "    make_word_cloud(listTfIdf_stars_body[i], max_words = n, title = \"TF-IDF body in %d /5 stars\" % i, mux = mux, mask_path = \"ImagesWordClouds/%dstars.jpg\" % i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "for i in range(1,6):\n",
    "    make_word_cloud({v:k for v,k in freq_dist_stars_body[i].most_common(n)}, max_words = n, title = \"Most common words body in %d /5 stars\" % i, mask_path = \"ImagesWordClouds/%dstars.jpg\" % i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the reviews' headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_dist_stars_head = dict()\n",
    "for i in range(1,6):\n",
    "    freq_dist_stars_head[i] = nltk.FreqDist(stars_head[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listTfIdf_stars_head = tfIdf(freq_dist_stars_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mux = len(all_reviews_without_stopwords) \n",
    "n = 200\n",
    "for i in range(1,6):\n",
    "    make_word_cloud(listTfIdf_stars_head[i], max_words = n, title = \"TF-IDF headline in %d /5 stars\" % i, mux = mux, mask_path = \"ImagesWordClouds/%dstars.jpg\" % i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "for i in range(1,6):\n",
    "    make_word_cloud({v:k for v,k in freq_dist_stars_head[i].most_common(n)}, max_words = n, title = \"Most common words headline in %d /5 stars\" % i, mask_path = \"ImagesWordClouds/%dstars.jpg\" % i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems it is far better to look at the headline to catch the \"sentiment\" of the reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
